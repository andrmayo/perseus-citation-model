{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "845a3be8-6cff-43a1-b337-f191f0521d91",
   "metadata": {},
   "source": [
    "# Citation Tagging End to End\n",
    "\n",
    "This notebook is for testing `src/perscit_model/xml_processing/tagger.py` as an application for taking in an XML file and inserting citation-relevant tags into it. Here, we take an XML file with reasonably accurate citation tags, strip it of these tags, reinsert them with the application, and then get the edit distance between the original version of the version with citations identified with the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e975956-1afb-4aa7-9d64-58abbeb2022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0a9f8eb-6ceb-4254-8f85-e2fcb6929f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from lxml import etree\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "\n",
    "from perscit_model.shared.xml_utils import strip_spec_elems, strip_spec_elem_attrs\n",
    "from perscit_model.xml_processing.tagger import CitationTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85112ae4-e738-43db-bd7e-e44e3fafb090",
   "metadata": {},
   "source": [
    "## Prepare files\n",
    "\n",
    "First, take XML files and make a copy with attribs stripped from citation-relevant tags. Then make another copy stripped of citation-relevant tags entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22a07c94-7eba-48fc-b078-57d4946e1a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('xml_original/campbell-sophlanguage-2.xml'), PosixPath('xml_original/viaf2603144.viaf001.perseus-eng1.xml')]\n"
     ]
    }
   ],
   "source": [
    "src_dir = Path(\"xml_original\")\n",
    "cmp_dir = src_dir.parent / \"xml_attr_stripped\"\n",
    "tagging_dir = src_dir.parent / \"xml_cit_stripped\"\n",
    "\n",
    "def prep_xml(path: Path) -> Path:\n",
    "    cmp_path = cmp_dir / path.name\n",
    "    tagging_path = tagging_dir / path.name\n",
    "    cit_tags = (\"cit\", \"bibl\", \"quote\")\n",
    "    \n",
    "    tree = etree.parse(path)\n",
    "    root = tree.getroot()\n",
    "    strip_spec_elem_attrs(root, cit_tags)\n",
    "    tree.write(\n",
    "        cmp_path, \n",
    "        encoding=tree.docinfo.encoding, \n",
    "        xml_declaration=tree.docinfo.xml_version is not None, \n",
    "        standalone=tree.docinfo.standalone\n",
    "    )\n",
    "    \n",
    "    tree = etree.parse(path)\n",
    "    root = tree.getroot()\n",
    "    strip_spec_elems(root, cit_tags)\n",
    "    tree.write(\n",
    "        tagging_path, \n",
    "        encoding=tree.docinfo.encoding,\n",
    "        xml_declaration=tree.docinfo.xml_version is not None, \n",
    "        standalone=tree.docinfo.standalone)\n",
    "\n",
    "cmp_dir.mkdir(exist_ok=True)\n",
    "tagging_dir.mkdir(exist_ok=True)\n",
    "src_paths = [file for file in src_dir.glob(\"*.xml\")]\n",
    "\n",
    "print(src_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f9bf65f-37d0-415e-91cc-8bafd15c31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in src_paths:\n",
    "    prep_xml(p)\n",
    "\n",
    "assert len(list(cmp_dir.glob(\"*.xml\"))) == len(src_paths)\n",
    "assert len(list(tagging_dir.glob(\"*.xml\"))) == len(src_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0bf504-5131-429c-9e4f-3ae3ae43d765",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now, we want to use the CitationTagger class to infer and insert `<cit>`, `<quote>`, and `<bibl>` tags into the XML files in `tagging_dir`. This should simply copy any existing citation tags, and ignore inferences that would overlap with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6946fde-7ce0-4cb6-b8e4-6b64e65a25d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/Projects/perseus-citation-model/.venv/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "XML validation failed for xml_cit_stripped/campbell-sophlanguage-2.xml, attempting to fix...\n",
      "Successfully recovered malformed XML using recovering parser in strip_citation_tags\n",
      "Successfully recovered malformed XML using recovering parser in strip_citation_tags\n",
      "XML validation failed for xml_cit_stripped/viaf2603144.viaf001.perseus-eng1.xml, attempting to fix...\n",
      "Successfully recovered malformed XML using recovering parser in strip_citation_tags\n",
      "Successfully recovered malformed XML using recovering parser in strip_citation_tags\n",
      "Base XML (without citations) is malformed for xml_cit_stripped/viaf2603144.viaf001.perseus-eng1.xml: Unescaped '<' not allowed in attributes values, line 1540, column 107 (<string>, line 1540)\n",
      "XML parsing failed for file xml_cit_stripped/viaf2603144.viaf001.perseus-eng1.xml, attempting recovery: Unescaped '<' not allowed in attributes values, line 1540, column 107 (<string>, line 1540)\n",
      "Successfully recovered malformed XML for file xml_cit_stripped/viaf2603144.viaf001.perseus-eng1.xml\n"
     ]
    }
   ],
   "source": [
    "model_path = Path(\"../outputs/models/extraction/\")\n",
    "tagger = CitationTagger(model_path)\n",
    "\n",
    "tagger.process_xml(tagging_dir, preserve_existing=True, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc548c-f8bb-47c7-a6d1-1fe6b607af2f",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Get normalized Levenshtein distance between files in `tagging_dir/processed` and original files (stripped of attributes), as well as between the files in `tagging_dir` stripped of all citation tags and the original files (stripped of attributes).\n",
    "\n",
    "We can use the `rapidfuzz` library to do this efficiently and fairly accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3dee13f-e301-4fb9-a177-d935c92f7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_levenshtein_large(a: Path, b: Path, chunk_size: int = 10000) -> float:\n",
    "    total_dist = 0.0\n",
    "    weight = 0\n",
    "    with open(a, \"r\") as f1, open(b, \"r\") as f2: \n",
    "        while True:\n",
    "            text_a = f1.read(chunk_size)\n",
    "            text_b = f2.read(chunk_size)\n",
    "\n",
    "            if (not text_a) and (not text_b):\n",
    "                break\n",
    "            total_dist += Levenshtein.distance(text_a, text_b)\n",
    "            weight += max(len(text_a), len(text_b))\n",
    "\n",
    "    return total_dist / weight if weight else 0.0\n",
    "\n",
    "prediction_distances = [\n",
    "    get_levenshtein_large(path_a, path_b) for path_a, path_b in zip(\n",
    "        (tagging_dir / \"processed\").glob(\"*.xml\"), cmp_dir.glob(\"*.xml\")\n",
    "    )\n",
    "]\n",
    "base_distances = [\n",
    "    get_levenshtein_large(path_a, path_b) for path_a, path_b in zip(\n",
    "        tagging_dir.glob(\"*.xml\"), cmp_dir.glob(\"*.xml\")\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c01492d0-82b3-4ba8-8c92-2b3e83914725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7721622497569245, 0.7368869962778417]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ed0db9c-143d-4a0a-8811-d7cb839ea07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.776722096183092, 0.7356525483875351]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c1b1e-df8b-4c10-873e-94a843cc9e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
