Metadata-Version: 2.4
Name: perseus-citation-model
Version: 0.1.0
Summary: Add your description here
Requires-Python: >=3.12
Description-Content-Type: text/markdown
Requires-Dist: bs4>=0.0.2
Requires-Dist: datasets>=4.4.1
Requires-Dist: lxml>=6.0.2
Requires-Dist: pytest>=9.0.1
Requires-Dist: torch>=2.9.1
Requires-Dist: transformers>=4.57.1

# Perseus Citation Model

Machine learning models for identifying citation structures in classical texts and resolving bibliographic references to canonical URNs.

## Overview

This project provides two complementary ML tasks for working with citations in TEI-encoded XML documents from the Perseus Digital Library:

1. **Tag Extraction**: Identify and extract citation tags (`<cit>`, `<quote>`, `<bibl>`) from plain text
2. **URN Resolution**: Map bibliographic references to Canonical Text Services (CTS) URNs

Both tasks share data pipelines and preprocessing infrastructure but use different model architectures appropriate to each problem.

## Task Definitions

### Task 1: Tag Extraction

**Input:** Plain text extracted from TEI XML documents
**Output:** Token-level tags identifying citation boundaries

**Target tags:**

- `<cit>` - Citation container
- `<quote>` - Quoted text
- `<bibl>` - Bibliographic reference

**Challenges:**

- Nested structures (e.g., `<cit>` contains `<quote>` and `<bibl>`)
- Variable citation formats
- Mixed languages (Greek, Latin, English)
- Context-dependent identification

### Task 2: URN Resolution

**Input:** Bibliographic reference text (e.g., "Hdt. 8.82")
**Output:** CTS URN (e.g., "urn:cts:greekLit:tlg0016.tlg001.perseus-grc2:8.82")

**Examples:**

- "Hom. Il. 7.268" â†’ "urn:cts:greekLit:tlg0012.tlg001.perseus-grc2:7.268"
- "Thuc. 3.38" â†’ "urn:cts:greekLit:tlg0003.tlg001.perseus-grc2:3.38"
- "Plat. Rep. 332D" â†’ "urn:cts:greekLit:tlg0059.tlg030.perseus-grc2:332d"

**Challenges:**

- Abbreviated author names (Hdt., Hom., Thuc.)
- Work title variations and abbreviations
- Range notation (e.g., "7.268-272", "sqq.")
- Unresolvable references to modern scholarship (e.g., "ARV2, 987")
- Missing URNs for ~12% of citations

## Data Format

Training data is in JSONL format with two files:

**`resolved.jsonl` (~216K examples)** - Citations with URNs:

```json
{
  "bibl": "Hdt. 8.82",
  "quote": "",
  "xml_context": "...full XML context with tags...",
  "filename": "xml_files/viaf17286815.viaf001.xml",
  "urn": "urn:cts:greekLit:tlg0016.tlg001.perseus-grc2:8.82",
  "ref": "hdt. 8.82",
  "n_attrib": "Hdt. 8.82",
  "doc_cit_urn": ":citations-28.3"
}
```

**`unresolved.jsonl` (~30K examples)** - Citations without URNs:

```json
{
  "bibl": "FR, pl. 167,2",
  "quote": "",
  "xml_context": "...full XML context with tags...",
  "filename": "xml_files/viaf114145308.viaf001.xml",
  "urn": "",
  "ref": "fr pl. 167,2",
  "n_attrib": "",
  "doc_cit_urn": ":citations-24.1"
}
```

**Key fields:**

- `bibl`: Bibliographic reference text
- `quote`: Quoted text (often empty)
- `xml_context`: XML snippet with tags for tag extraction training
- `urn`: CTS URN (empty for unresolved citations)
- `ref`: Normalized reference text

---

# Task 1: Tag Extraction

## Approach 1: Transformer-based Token Classification

### Overview

Fine-tune a pre-trained transformer model (DeBERTa, RoBERTa, or BERT) for sequence labeling using BIO tagging.

### Architecture

```
Input Text â†’ Tokenizer â†’ Transformer Encoder â†’ Linear Layer â†’ Softmax â†’ BIO Tags
```

### BIO Tagging Scheme

Each token is labeled with one of:

- `O` - Outside any citation tag
- `B-CIT` - Beginning of `<cit>` tag
- `I-CIT` - Inside `<cit>` tag
- `B-QUOTE` - Beginning of `<quote>` tag
- `I-QUOTE` - Inside `<quote>` tag
- `B-BIBL` - Beginning of `<bibl>` tag
- `I-BIBL` - Inside `<bibl>` tag

**Example:**

```
Text:     Hom.  Il.  7.268  -  272  :  "Ajax  hurled  a  rock"
Tags:     B-BIBL I-BIBL I-BIBL I-BIBL I-BIBL I-BIBL O B-QUOTE I-QUOTE I-QUOTE I-QUOTE
```

### Model Selection

**Recommended models (in order of preference):**

1. **`microsoft/deberta-v3-base`** - Best for this task due to:
   - Superior contextual understanding for nested structures
   - Better multilingual handling (Greek, Latin, English)
   - State-of-the-art performance on token classification
   - 1-3% F1 improvement over RoBERTa on similar tasks

2. **`roberta-base`** - Good alternative if:
   - Need faster inference (~10-15% faster than DeBERTa)
   - Memory constraints
   - Strong baseline performance

3. **`bert-base-uncased`** - Use only for:
   - Quick prototyping
   - Establishing baseline metrics

### Implementation

```python
from transformers import (
    AutoTokenizer,
    AutoModelForTokenClassification,
    Trainer,
    TrainingArguments,
    DataCollatorForTokenClassification
)
from datasets import Dataset
import torch

# 1. Define label set
labels = ["O", "B-CIT", "I-CIT", "B-QUOTE", "I-QUOTE", "B-BIBL", "I-BIBL"]
label2id = {label: i for i, label in enumerate(labels)}
id2label = {i: label for i, label in enumerate(labels)}

# 2. Load model
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(
    model_name,
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id
)

# 3. Prepare dataset
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"],
        truncation=True,
        is_split_into_words=True,
        padding=True
    )

    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        label_ids = []
        previous_word_idx = None

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore special tokens
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            else:
                # For subword tokens, use same label or -100
                label_ids.append(label[word_idx])
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Load your dataset
train_dataset = Dataset.from_dict({
    "tokens": [...],      # List of token lists
    "ner_tags": [...]     # List of label ID lists
})

tokenized_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)

# 4. Training configuration
training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=False,
)

# 5. Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorForTokenClassification(tokenizer),
)

trainer.train()
```

### Pros

- Simple to implement with HuggingFace
- Strong baseline performance
- Fast training with GPU
- Well-documented and widely used
- Transfer learning from pre-trained models

### Cons

- May produce invalid tag sequences (e.g., `I-QUOTE` without `B-QUOTE`)
- Doesn't explicitly model tag dependencies
- Treats each token prediction independently

---

## Approach 2: Transformer + CRF (Hybrid)

### Overview

Combines transformer contextual embeddings with a Conditional Random Field (CRF) layer to ensure valid tag sequences.

### Architecture

```
Input Text â†’ Tokenizer â†’ Transformer Encoder â†’ Linear Layer â†’ CRF Layer â†’ BIO Tags
```

### Why Add CRF?

The CRF layer learns transition probabilities between tags, ensuring:

- Valid sequences (e.g., `I-QUOTE` must follow `B-QUOTE` or `I-QUOTE`)
- No impossible transitions (e.g., `B-CIT` â†’ `I-QUOTE` directly)
- Global optimization across the entire sequence
- Structured prediction with dependencies

### Implementation

```python
from transformers import AutoModel, AutoTokenizer
import torch
import torch.nn as nn
from torchcrf import CRF

class BertCRF(nn.Module):
    def __init__(self, model_name, num_labels):
        super().__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        self.crf = CRF(num_labels, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs[0]
        sequence_output = self.dropout(sequence_output)
        emissions = self.classifier(sequence_output)

        if labels is not None:
            # Training: compute CRF loss
            loss = -self.crf(emissions, labels, mask=attention_mask.byte(), reduction='mean')
            return loss
        else:
            # Inference: Viterbi decoding
            predictions = self.crf.decode(emissions, mask=attention_mask.byte())
            return predictions

# Initialize model
labels = ["O", "B-CIT", "I-CIT", "B-QUOTE", "I-QUOTE", "B-BIBL", "I-BIBL"]
model = BertCRF("bert-base-uncased", num_labels=len(labels))

# Training loop
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

for epoch in range(num_epochs):
    model.train()
    for batch in train_dataloader:
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        loss = model(input_ids, attention_mask, labels)
        loss.backward()
        optimizer.step()

# Inference
model.eval()
with torch.no_grad():
    predictions = model(input_ids, attention_mask)
```

### CRF Layer Details

**What CRF learns:**

- Transition matrix: probability of tag A followed by tag B
- Start transitions: which tags can begin a sequence
- End transitions: which tags can end a sequence

**Example constraints:**

```
Valid:   B-QUOTE â†’ I-QUOTE â†’ I-QUOTE â†’ O
Valid:   O â†’ B-BIBL â†’ I-BIBL â†’ O
Invalid: O â†’ I-QUOTE (can't start with I-)
Invalid: B-QUOTE â†’ I-BIBL (can't switch tag types)
```

### Installation

```bash
pip install pytorch-crf
# or
pip install torchcrf
```

### Pros

- Guaranteed valid tag sequences
- Models dependencies between adjacent tags
- Often 1-3% better F1 score than plain transformers
- Theoretically sound structured prediction
- Better handling of rare tag transitions

### Cons

- More complex implementation
- Slightly slower training/inference
- Additional hyperparameters to tune
- Requires more memory (stores transition matrix)

---

## Comparison

| Feature               | Transformer Only             | Transformer + CRF        |
| --------------------- | ---------------------------- | ------------------------ |
| **Implementation**    | Simple (HuggingFace Trainer) | Moderate (custom model)  |
| **Training Speed**    | Fast                         | 10-20% slower            |
| **Sequence Validity** | No guarantee                 | Guaranteed               |
| **Performance**       | Good baseline                | Usually 1-3% better      |
| **Memory**            | Lower                        | Higher                   |
| **Best for**          | Quick experiments, baseline  | Production, max accuracy |

---

## Data Preparation Pipeline

### 1. Extract and Parse JSONL

```python
import json
import re
from bs4 import BeautifulSoup

def load_data(jsonl_path):
    """Load training data from JSONL file."""
    examples = []
    with open(jsonl_path, 'r') as f:
        for line in f:
            examples.append(json.loads(line))
    return examples

def extract_tags_from_xml(xml_context):
    """Extract text and BIO tags from XML context."""
    soup = BeautifulSoup(xml_context, 'xml')
    tokens = []
    labels = []

    # Parse XML and extract tokens with their tags
    # This is a simplified example - needs proper implementation
    for elem in soup.descendants:
        if isinstance(elem, str):
            text = elem.strip()
            if text:
                words = text.split()
                parent_tag = elem.parent.name if elem.parent else None

                for i, word in enumerate(words):
                    tokens.append(word)
                    if parent_tag == 'cit':
                        labels.append('B-CIT' if i == 0 else 'I-CIT')
                    elif parent_tag == 'quote':
                        labels.append('B-QUOTE' if i == 0 else 'I-QUOTE')
                    elif parent_tag == 'bibl':
                        labels.append('B-BIBL' if i == 0 else 'I-BIBL')
                    else:
                        labels.append('O')

    return tokens, labels
```

### 2. Handle Nested Tags

For nested structures like:

```xml
<cit>
  <bibl>Hdt. 8.82</bibl>
  <quote>Ajax hurled a rock</quote>
</cit>
```

**Strategy options:**

1. **Innermost wins:** Use most specific tag (`BIBL`/`QUOTE` over `CIT`)
2. **Multi-label:** Predict multiple tags per token
3. **Hierarchical:** Separate models for each level
4. **Flatten:** Only tag innermost meaningful elements

### 3. Split Dataset

```python
from sklearn.model_selection import train_test_split

# Split by document to avoid leakage
train_docs, test_docs = train_test_split(
    documents,
    test_size=0.2,
    random_state=42
)
train_docs, val_docs = train_test_split(
    train_docs,
    test_size=0.1,
    random_state=42
)
```

---

## Evaluation Metrics

```python
from seqeval.metrics import classification_report, f1_score

# seqeval properly handles BIO tagging
predictions = ["O", "B-BIBL", "I-BIBL", "O", "B-QUOTE", "I-QUOTE"]
ground_truth = ["O", "B-BIBL", "I-BIBL", "O", "B-QUOTE", "I-QUOTE"]

print(classification_report([ground_truth], [predictions]))
```

**Key metrics:**

- **Precision:** Of predicted tags, how many are correct?
- **Recall:** Of actual tags, how many did we find?
- **F1-score:** Harmonic mean of precision and recall
- **Entity-level F1:** Full span must match (stricter)

---

## Training Recommendations

### Hyperparameters

**Transformer only:**

- Learning rate: 2e-5 to 5e-5
- Batch size: 16-32
- Epochs: 3-5
- Warmup steps: 500
- Weight decay: 0.01

**Transformer + CRF:**

- Learning rate: 5e-5 (CRF may need slightly higher)
- Batch size: 16 (CRF uses more memory)
- Epochs: 3-5
- CRF learning rate: Can be different from BERT (e.g., 1e-3)

### Hardware Requirements

**Minimum:**

- GPU: 8GB VRAM (e.g., RTX 2070, T4)
- RAM: 16GB
- Storage: 10GB for model + data

**Recommended:**

- GPU: 16GB+ VRAM (e.g., V100, A100, RTX 3090)
- RAM: 32GB
- Storage: 50GB

### Training Time Estimates

With ~200MB JSONL data on single GPU:

- Data preparation: 1-2 hours
- Transformer training (3 epochs): 2-4 hours
- Transformer+CRF training (3 epochs): 3-5 hours

---

# Task 2: URN Resolution

## Overview

URN resolution maps bibliographic reference strings to Canonical Text Services (CTS) URNs. This is a different ML problem from tag extraction - it's a **structured prediction** or **sequence-to-sequence** task rather than token classification.

## Recommended Approaches

### Approach 1: Rule-based + Hierarchical Classification (Recommended)

**Strategy:** Use perseus-citation-processor for 87.6% of cases, hierarchical DNN classifiers for the remaining 12.4%.

**Advantages:**

- **Guaranteed valid URNs** - classification over known catalog, no hallucination
- **Interpretable** - see which stage (author/work) succeeded or failed
- **Efficient** - small vocabularies (~500 authors, ~50 works per author)
- **High precision** - rule-based handles well-formatted citations (87.6%)
- **Debuggable** - can inspect author confidence vs work confidence separately

**Architecture:**

```
Input: "Hdt. 8.82"
  â†“
[perseus-citation-processor] â†’ 87.6% resolved directly
  â†“ (if unresolved or low confidence)
[Author Classifier (DNN)] â†’ tlg0016 [conf: 0.95]
  â†“
[Work Classifier (DNN)] â†’ tlg0016.tlg001 [conf: 0.92]
  â†“
[Passage Parser (rules)] â†’ 8.82
  â†“
[Edition Selector (rules)] â†’ perseus-grc2
  â†“
Assemble: "urn:cts:greekLit:tlg0016.tlg001.perseus-grc2:8.82"
```

**Components:**

1. **Rule-based baseline**: perseus-citation-processor (Go binary)
2. **Author classifier**: DeBERTa classification over ~500 Greek/Latin authors
3. **Work classifier**: DeBERTa classification over works (conditioned on author)
4. **Passage parser**: Rule-based extraction of passage references
5. **Edition selector**: Rule-based (Greek quote â†’ grc, Latin quote â†’ lat)
6. **Confidence scorer**: DeBERTa binary classifier for quality control

### Approach 2: Sequence-to-Sequence Model (NOT Recommended)

**Models:** T5, BART, ByT5, or encoder-decoder transformers

**Why NOT recommended:**

- **URNs are structured catalog lookups**, not creative text generation
- **Can hallucinate** invalid author/work combinations
- **Less interpretable** - black box generation
- **Wasteful** - learns URN syntax instead of just citationâ†’URN mappings
- **No guaranteed validity** - requires complex constrained decoding

**When to consider:**

- If you need to handle completely new authors not in any catalog (unlikely for classical texts)
- If you want to experiment with end-to-end learning
- As a baseline to compare against hierarchical classification

**Architecture:**

```python
from transformers import T5ForConditionalGeneration, AutoTokenizer

# ByT5 for character-level handling of Greek/Latin
model = T5ForConditionalGeneration.from_pretrained("google/byt5-base")
tokenizer = AutoTokenizer.from_pretrained("google/byt5-base")

# Training format: "resolve citation: Hdt. 8.82" â†’ "urn:cts:greekLit:..."
input_text = "resolve citation: Hdt. 8.82"
inputs = tokenizer(input_text, return_tensors="pt")
outputs = model.generate(**inputs)

# Major issue: May generate syntactically valid but semantically wrong URNs
# e.g., "urn:cts:greekLit:tlg9999.tlg999.perseus-grc2:8.82" (invalid author)
```

**Verdict:** Use hierarchical classification (Approach 1) instead.

### Approach 3: Retrieval-Augmented Generation

**Strategy:** Retrieve similar citations from database, use ML to rank/select.

**Advantages:**

- Leverages existing resolved citations
- Good for rare author/work combinations
- Can explain predictions via retrieved examples

**Architecture:**

```
Input: "Thuc. 3.38"
  â†“
Embedding model â†’ Dense vector representation
  â†“
Vector DB â†’ Retrieve top-K similar resolved citations
  â†“
Ranking model â†’ Score candidates
  â†“
Output: Highest-scoring URN
```

---

## Implementation: Hybrid Rule-based + DNN System

**Recommended Approach:** Hierarchical Classification (NOT seq2seq)

This implementation uses:

1. **Rule-based baseline** (perseus-citation-processor) for 87.6% of citations
2. **Hierarchical DNN classifiers** for the remaining 12.4%
   - Author classifier (DeBERTa)
   - Work classifier (DeBERTa, conditioned on author)
3. **Confidence scorer** (DeBERTa) to validate rule-based outputs

**Why hierarchical classification over seq2seq?**

- Guaranteed valid URNs (no hallucination)
- Interpretable decisions (author vs work failures)
- Efficient (small vocabularies)
- Better uncertainty handling (top-k at each stage)

### Foundation: perseus-citation-processor

This project uses the **[perseus-citation-processor](https://github.com/andrewbird2/perseus-citation-processor)** as the rule-based foundation:

**Performance on 246K citations:**

- âœ… **87.6% resolution rate** (216K resolved)
- âŒ **12.4% unresolved** (30K citations)
- âš¡ Fast: 28 seconds for 125 XML files
- ğŸ“š Comprehensive author/work mappings (Greek, Latin, Scholia)

The DNN components handle the remaining 12.4% plus add confidence scoring.

### Architecture Overview

```
Input Citation
       â†“
[perseus-citation-processor] â† Rule-based (87.6% coverage)
       â†“
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”
   â†“        â†“
Resolved  Unresolved
   â†“        â†“
[Confidence  [DNN Resolution
 Scorer]      Model]
   â†“        â†“
High conf?  URN + conf
   â†“        â†“
Yesâ†’Output  Mergeâ†’Output
   Noâ†˜    â†—
    [DNN Model]
```

### DNN Component 1: Unresolved Citation Resolver

**Purpose:** Resolve the 30K citations that rule-based system couldn't handle.

**Approach:** Hierarchical Classification (NOT seq2seq generation)

**Why Classification over Seq2Seq:**

- **URNs are structured catalog lookups**, not creative text
- **Guaranteed valid outputs** - cannot hallucinate invalid author/work combinations
- **Interpretable** - see exactly which stage succeeded/failed
- **Efficient** - small vocabulary (~500 authors Ã— ~50 works = 25K combinations)
- **Better uncertainty handling** - top-k predictions at each stage

**Architecture:** Multi-stage classification pipeline

```
"Hdt. 8.82"
    â†“
[Author Classifier] â†’ tlg0016 (Herodotus) [confidence: 0.95]
    â†“
[Work Classifier] â†’ tlg0016.tlg001 (Histories) [confidence: 0.92]
    â†“
[Edition Selector] â†’ perseus-grc2 (rule-based: Greek text â†’ grc)
    â†“
[Passage Parser] â†’ 8.82 (rule-based extraction)
    â†“
Assemble: "urn:cts:greekLit:tlg0016.tlg001.perseus-grc2:8.82"
```

**Implementation:**

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

class AuthorClassifier:
    """Stage 1: Classify citation to author URN"""

    def __init__(self):
        # Load perseus-citation-processor author catalog
        self.author_catalog = self.load_author_catalog()  # ~500 Greek/Latin authors
        self.author_to_id = {urn: i for i, urn in enumerate(self.author_catalog)}
        self.id_to_author = {i: urn for urn, i in self.author_to_id.items()}

        # DeBERTa classifier over author vocabulary
        self.model = AutoModelForSequenceClassification.from_pretrained(
            "microsoft/deberta-v3-base",
            num_labels=len(self.author_catalog)
        )
        self.tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-v3-base")

    def predict(self, citation_text, context=""):
        """Predict top-k authors with confidence scores"""
        # Input format: "[citation] [SEP] [context]"
        text = f"{citation_text} [SEP] {context}"
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)

        # Get predictions
        outputs = self.model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1)
        top_k = torch.topk(probs[0], k=3)

        # Return [(author_urn, confidence), ...]
        return [(self.id_to_author[idx.item()], prob.item())
                for idx, prob in zip(top_k.indices, top_k.values)]


class WorkClassifier:
    """Stage 2: Classify to work URN (conditioned on author)"""

    def __init__(self):
        # Group works by author
        self.works_by_author = self.load_work_catalog()  # {tlg0016: [tlg001, tlg002, ...]}
        self.max_works = max(len(works) for works in self.works_by_author.values())

        # Classifier conditioned on author
        self.model = AutoModelForSequenceClassification.from_pretrained(
            "microsoft/deberta-v3-base",
            num_labels=self.max_works
        )
        self.tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-v3-base")

    def predict(self, citation_text, author_urn, context=""):
        """Predict work for given author"""
        # Get candidate works for this author
        candidates = self.works_by_author.get(author_urn, [])
        if not candidates:
            return []

        # Input format: "[author] [SEP] [citation] [SEP] [context]"
        text = f"{author_urn} [SEP] {citation_text} [SEP] {context}"
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)

        outputs = self.model(**inputs)

        # Filter to only valid works for this author
        valid_logits = outputs.logits[0, :len(candidates)]
        probs = torch.softmax(valid_logits, dim=-1)

        # Return [(work_urn, confidence), ...] sorted by confidence
        results = [(candidates[i], probs[i].item()) for i in range(len(candidates))]
        return sorted(results, key=lambda x: x[1], reverse=True)


class HierarchicalURNResolver:
    """Complete hierarchical URN resolution system"""

    def __init__(self):
        self.author_classifier = AuthorClassifier()
        self.work_classifier = WorkClassifier()
        self.passage_parser = PassageParser()  # Rule-based
        self.edition_selector = EditionSelector()  # Rule-based

    def resolve(self, citation_text, context="", quote=""):
        """Resolve citation to URN with confidence score"""
        # Stage 1: Classify author
        author_candidates = self.author_classifier.predict(citation_text, context)

        # Stage 2: For each author candidate, classify work
        urn_candidates = []
        for author_urn, author_conf in author_candidates[:3]:  # Top-3 authors
            work_candidates = self.work_classifier.predict(
                citation_text, author_urn, context
            )

            for work_urn, work_conf in work_candidates[:3]:  # Top-3 works
                # Stage 3: Parse passage (rule-based)
                passage = self.passage_parser.extract(citation_text)

                # Stage 4: Select edition (rule-based)
                edition = self.edition_selector.select(author_urn, quote)

                # Stage 5: Assemble URN
                namespace = self.get_namespace(author_urn)  # greekLit or latinLit
                full_urn = f"urn:cts:{namespace}:{work_urn}.{edition}:{passage}"

                # Combined confidence (product of stages)
                confidence = author_conf * work_conf

                urn_candidates.append((full_urn, confidence, {
                    'author_conf': author_conf,
                    'work_conf': work_conf,
                    'author': author_urn,
                    'work': work_urn
                }))

        # Return best candidate
        if urn_candidates:
            best = max(urn_candidates, key=lambda x: x[1])
            return best[0], best[1], best[2]
        else:
            return None, 0.0, {}
```

**Training Data Format:**

```python
# Training examples from resolved.jsonl (216K examples)
# Split into author and work classification tasks

# Stage 1: Author Classification
author_examples = [
    {
        "citation": "Hdt. 8.82",
        "context": "",
        "label": "tlg0016"  # Herodotus
    },
    {
        "citation": "Soph. OT 151",
        "context": "Ï„á¾¶Ï‚ Ï€Î¿Î»Ï…Ï‡ÏÏÏƒÎ¿Ï… Î Ï…Î¸á¿¶Î½Î¿Ï‚",
        "label": "tlg0011"  # Sophocles
    },
    {
        "citation": "Plat. Rep. 332D",
        "context": "",
        "label": "tlg0059"  # Plato
    }
]

# Stage 2: Work Classification (conditioned on author)
work_examples = [
    {
        "citation": "Hdt. 8.82",
        "author": "tlg0016",
        "context": "",
        "label": "tlg0016.tlg001"  # Histories
    },
    {
        "citation": "Soph. OT 151",
        "author": "tlg0011",
        "context": "Ï„á¾¶Ï‚ Ï€Î¿Î»Ï…Ï‡ÏÏÏƒÎ¿Ï… Î Ï…Î¸á¿¶Î½Î¿Ï‚",
        "label": "tlg0011.tlg004"  # Oedipus Tyrannus
    },
    {
        "citation": "Hom. Il. 7.268",
        "author": "tlg0012",
        "context": "Ajax hurled a rock",
        "label": "tlg0012.tlg001"  # Iliad (not Odyssey)
    }
]
```

**Features Used:**

1. **Citation text** (required): "Hdt. 8.82"
2. **Context** (when available): Surrounding text or quote
3. **Author URN** (for work classification): Condition on stage 1 output
4. **Ground truth URN**: Parse author/work from `urn` field

### DNN Component 2: Confidence Scorer

**Purpose:** Identify incorrect resolutions from perseus-citation-processor.

**Problem:** From perseus-citation-processor README: "just because the processor resolves a citation doesn't mean that it resolves it correctly"

**Model:** DeBERTa-based Binary Classifier

```python
from transformers import AutoModel, AutoTokenizer
import torch.nn as nn

class URNConfidenceScorer(nn.Module):
    def __init__(self, model_name="microsoft/deberta-v3-base"):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.1)

        hidden_size = self.encoder.config.hidden_size
        feature_size = 10  # Engineered features

        self.classifier = nn.Sequential(
            nn.Linear(hidden_size + feature_size, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, citation_text, urn, context=""):
        # Encode: "[citation] <SEP> [URN] <SEP> [context]"
        text = f"{citation_text} [SEP] {urn} [SEP] {context}"
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True)

        outputs = self.encoder(**inputs)
        pooled = outputs.last_hidden_state[:, 0, :]  # CLS token

        # Compute engineered features
        features = self.compute_features(citation_text, urn, context)

        combined = torch.cat([pooled, features], dim=1)
        confidence = self.classifier(combined)

        return confidence

    def compute_features(self, citation, urn, context):
        """Engineered features for confidence scoring"""
        return torch.tensor([
            self.citation_urn_match_score(citation, urn),
            self.has_ambiguous_abbrev(citation),
            self.context_language_match(context, urn),
            self.passage_validity(urn),
            self.author_frequency(urn),
            self.work_frequency(urn),
            self.quote_presence(context),
            self.citation_length(citation),
            self.urn_complexity(urn),
            self.catalog_match_count(citation)
        ])
```

**Training Data:**

```python
# Positive examples (confident resolutions)
positive_examples = [
    {
        "citation": "Hdt. 8.82",
        "urn": "urn:cts:greekLit:tlg0016.tlg001.perseus-grc2:8.82",
        "context": "Greek text context",
        "label": 1.0  # High confidence
    }
]

# Ambiguous cases (medium confidence)
ambiguous_examples = [
    {
        "citation": "Arist. Met.",  # Metaphysics or Meteorology?
        "urn": "urn:cts:greekLit:tlg0086.tlg025.perseus-grc2:",
        "context": "",
        "label": 0.6  # Medium confidence
    }
]

# Incorrect resolutions (low confidence)
negative_examples = [
    {
        "citation": "Hom. 7.268",  # Iliad or Odyssey?
        "urn": "urn:cts:greekLit:tlg0012.tlg002.perseus-grc2:7.268",  # Odyssey
        "context": "Ajax hurled a rock",  # Ajax is in Iliad!
        "label": 0.1  # Low confidence - context mismatch
    }
]
```

### DNN Component 3: Hybrid Orchestrator

**Purpose:** Combine rule-based and DNN intelligently.

```python
class HybridURNResolver:
    def __init__(self):
        self.rule_based = PerseusProcessorWrapper()  # Calls Go binary
        self.dnn_resolver = HierarchicalURNResolver()  # Hierarchical classifier
        self.confidence_scorer = URNConfidenceScorer()

        # Thresholds (tune on validation set)
        self.high_confidence_threshold = 0.85
        self.low_confidence_threshold = 0.50

    def resolve(self, citation_text, context="", quote=""):
        # Step 1: Try rule-based
        rule_urn, rule_status = self.rule_based.resolve(citation_text)

        # Step 2: Score rule-based result
        if rule_urn:
            confidence = self.confidence_scorer(citation_text, rule_urn, context)

            if confidence > self.high_confidence_threshold:
                return rule_urn, confidence, "rule-based"

            # Medium confidence - get DNN opinion
            elif confidence > self.low_confidence_threshold:
                dnn_urn, dnn_conf, dnn_details = self.dnn_resolver.resolve(
                    citation_text, context, quote
                )

                # Compare: rule-based vs DNN
                if dnn_conf > confidence:
                    return dnn_urn, dnn_conf, "dnn-override", dnn_details
                else:
                    return rule_urn, confidence, "rule-based-verified"

            # Low confidence - prefer DNN
            else:
                dnn_urn, dnn_conf, dnn_details = self.dnn_resolver.resolve(
                    citation_text, context, quote
                )
                return dnn_urn, dnn_conf, "dnn-preferred", dnn_details

        # Step 3: Rule-based failed, use DNN only
        else:
            dnn_urn, dnn_conf, dnn_details = self.dnn_resolver.resolve(
                citation_text, context, quote
            )
            return dnn_urn, dnn_conf, "dnn-only", dnn_details
```

### Training Strategy

**Phase 1: Train Author Classifier**

```python
from datasets import Dataset
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainingArguments
)
import json

# Step 1: Extract author labels from URNs
def parse_urn_to_author(urn):
    """Extract author URN from full CTS URN"""
    # urn:cts:greekLit:tlg0016.tlg001.perseus-grc2:8.82 â†’ tlg0016
    try:
        parts = urn.split(":")
        author_work = parts[3]  # tlg0016.tlg001.perseus-grc2
        author = author_work.split(".")[0]  # tlg0016
        return author
    except:
        return None

# Step 2: Build author vocabulary from perseus-citation-processor data
author_catalog = set()
with open("cit_data/resolved.jsonl") as f:
    for line in f:
        item = json.loads(line)
        author = parse_urn_to_author(item['urn'])
        if author:
            author_catalog.add(author)

author_to_id = {author: i for i, author in enumerate(sorted(author_catalog))}
id_to_author = {i: author for author, i in author_to_id.items()}

# Step 3: Prepare training data
train_data = []
with open("cit_data/resolved.jsonl") as f:
    for line in f:
        item = json.loads(line)
        author = parse_urn_to_author(item['urn'])
        if author and author in author_to_id:
            train_data.append({
                "text": f"{item['bibl']} [SEP] {item.get('quote', '')}",
                "label": author_to_id[author]
            })

# Train/val split
dataset = Dataset.from_list(train_data)
dataset = dataset.train_test_split(test_size=0.1)

# Step 4: Train DeBERTa classifier
model = AutoModelForSequenceClassification.from_pretrained(
    "microsoft/deberta-v3-base",
    num_labels=len(author_catalog)
)
tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-v3-base")

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

training_args = TrainingArguments(
    output_dir="./outputs/resolution/models/author-classifier",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    num_train_epochs=3,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset['train'],
    eval_dataset=tokenized_dataset['test'],
)

trainer.train()
```

**Phase 2: Train Work Classifier**

```python
# Step 1: Parse work labels from URNs
def parse_urn_to_work(urn):
    """Extract work URN from full CTS URN"""
    # urn:cts:greekLit:tlg0016.tlg001.perseus-grc2:8.82 â†’ tlg0016.tlg001
    try:
        parts = urn.split(":")
        author_work = parts[3]  # tlg0016.tlg001.perseus-grc2
        work = ".".join(author_work.split(".")[:2])  # tlg0016.tlg001
        return work
    except:
        return None

# Step 2: Build work vocabulary grouped by author
works_by_author = {}
with open("cit_data/resolved.jsonl") as f:
    for line in f:
        item = json.loads(line)
        author = parse_urn_to_author(item['urn'])
        work = parse_urn_to_work(item['urn'])
        if author and work:
            if author not in works_by_author:
                works_by_author[author] = set()
            works_by_author[author].add(work)

# Step 3: Prepare training data (conditioned on author)
train_data = []
with open("cit_data/resolved.jsonl") as f:
    for line in f:
        item = json.loads(line)
        author = parse_urn_to_author(item['urn'])
        work = parse_urn_to_work(item['urn'])

        if author and work:
            # Input includes author URN to condition on
            train_data.append({
                "text": f"{author} [SEP] {item['bibl']} [SEP] {item.get('quote', '')}",
                "author": author,
                "label": work
            })

# Create label mapping (per-author work indices)
# ... (similar training setup)
```

**Phase 3: Train Confidence Scorer**

- Curate labeled examples with confidence scores
- Use cross-validation on resolved.jsonl
- Add manually labeled ambiguous cases (~1K examples)
- Generate synthetic negative examples (incorrect author/work pairs)

**Phase 4: Evaluate Hierarchical Classifier**

- Test on held-out validation set (10% of resolved.jsonl)
- Measure component accuracy:
  - Author classification accuracy
  - Work classification accuracy (given correct author)
  - End-to-end URN exact match
- Test on 30K unresolved.jsonl examples
- Manual evaluation on random sample for accuracy

### Expected Performance

**Conservative estimates:**

| Component                                    | Coverage Improvement |
| -------------------------------------------- | -------------------- |
| Rule-based baseline                          | 87.6%                |
| DNN on unresolved (30-50% success)           | +4-6%                |
| Confidence filtering (catch incorrect rules) | +2-3%                |
| **Total hybrid coverage**                    | **~94-97%**          |

**Key insight:** Even resolving 1/3 of unresolved cases is significant improvement.

## Evaluation Metrics

**For URN Resolution:**

- **Exact match accuracy**: Percentage of perfectly resolved URNs
- **Component accuracy**: Separate metrics for author, work, passage
- **Coverage**: Percentage of citations with confident predictions
- **Precision@K**: Accuracy when allowing top-K predictions

## Data Split Considerations

Unlike tag extraction, URN resolution should be split by **unique citation patterns** rather than documents to test generalization:

- Test on unseen author abbreviations
- Test on unseen work titles
- Test on seen authors but unseen works

---

## Next Steps

### Tag Extraction

1. **Data exploration:** Analyze `resolved.jsonl` statistics
2. **Preprocessing:** Build data pipeline from JSONL to BIO format
3. **Baseline model:** Train DeBERTa token classifier
4. **Evaluation:** Measure performance on held-out test set
5. **Advanced model:** Implement DeBERTa+CRF if baseline is insufficient
6. **Error analysis:** Examine failure cases

### URN Resolution

**Phase 1: Rule-based Baseline (Week 1-2)**

1. **Integration:** Wrap perseus-citation-processor as Python callable
2. **Baseline metrics:** Establish 87.6% resolution rate on 216K citations
3. **Error analysis:** Analyze 30K unresolved.jsonl patterns
4. **Catalog extraction:** Load author/work URNs from perseus-citation-processor data

**Phase 2: Hierarchical Classifiers (Week 3-5)**

1. **Catalog extraction:** Extract author/work vocabularies from resolved.jsonl URNs
2. **Train author classifier:** DeBERTa classification over ~500 authors
   - Input: citation + context
   - Output: author URN (e.g., tlg0016)
3. **Train work classifier:** DeBERTa classification conditioned on author
   - Input: author + citation + context
   - Output: work URN (e.g., tlg0016.tlg001)
4. **Implement passage parser:** Rule-based extraction of passage references
5. **Evaluate components:** Measure author accuracy, work accuracy, end-to-end
6. **Test on unresolved:** Evaluate on 30K unresolved.jsonl citations

**Phase 3: Confidence Scorer (Week 5-6)**

1. **Training data curation:** Label confident vs ambiguous resolutions
2. **Train DeBERTa classifier:** (citation, URN, context) â†’ confidence score
3. **Threshold tuning:** Calibrate confidence thresholds on validation set
4. **Cross-validation:** Test on ambiguous cases from resolved.jsonl

**Phase 4: Hybrid System (Week 7-8)**

1. **Orchestrator implementation:** Combine rule-based + DNN components
2. **End-to-end evaluation:** Measure coverage improvement (target: 94-97%)
3. **Component analysis:** Track rule-based vs DNN vs hybrid performance
4. **Error analysis:** Identify remaining failure modes

---

## Project Structure

```
perseus-citation-model/
â”‚
â”œâ”€â”€ cit_data/                          # Raw training data
â”‚   â”œâ”€â”€ resolved.jsonl                 # 216K examples - main training data
â”‚   â”œâ”€â”€ unresolved.jsonl               # 30K examples
â”‚   â””â”€â”€ document_mappings.json
â”‚
â”œâ”€â”€ model_data/                        # Processed datasets
â”‚   â”œâ”€â”€ processed/                     # BIO-tagged data ready for training
â”‚   â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”‚   â”œâ”€â”€ val.jsonl
â”‚   â”‚   â””â”€â”€ test.jsonl
â”‚   â”œâ”€â”€ splits/                        # Document-level split indices
â”‚   â”‚   â””â”€â”€ split_config.json
â”‚   â””â”€â”€ stats/                         # Data statistics and analysis
â”‚       â””â”€â”€ data_summary.json
â”‚
â”œâ”€â”€ src/                               # Source code
â”‚   â””â”€â”€ perscit_model/                # Main package
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ shared/                    # Shared utilities across both tasks
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ data_loader.py         # Load JSONL files
â”‚       â”‚   â”œâ”€â”€ preprocessing.py       # Common text preprocessing
â”‚       â”‚   â””â”€â”€ utils.py               # Shared helper functions
â”‚       â”œâ”€â”€ extraction/                # Task 1: Tag Extraction
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ data/                  # Tag extraction data processing
â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”‚   â”œâ”€â”€ xml_parser.py      # Parse XML context, extract BIO tags
â”‚       â”‚   â”‚   â”œâ”€â”€ dataset.py         # PyTorch Dataset for token classification
â”‚       â”‚   â”‚   â””â”€â”€ collator.py        # Data collators
â”‚       â”‚   â”œâ”€â”€ models/                # Tag extraction models
â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”‚   â”œâ”€â”€ transformer_tagger.py  # DeBERTa/RoBERTa token classifier
â”‚       â”‚   â”‚   â””â”€â”€ transformer_crf.py     # Transformer + CRF model
â”‚       â”‚   â”œâ”€â”€ training/              # Training logic
â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”‚   â”œâ”€â”€ trainer.py         # Training loops
â”‚       â”‚   â”‚   â””â”€â”€ callbacks.py       # Custom callbacks
â”‚       â”‚   â””â”€â”€ evaluation/            # Evaluation and metrics
â”‚       â”‚       â”œâ”€â”€ __init__.py
â”‚       â”‚       â”œâ”€â”€ metrics.py         # seqeval metrics, BIO scoring
â”‚       â”‚       â””â”€â”€ analysis.py        # Error analysis utilities
â”‚       â””â”€â”€ resolution/                # Task 2: URN Resolution
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ parsing/               # Citation parsing
â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚           â”‚   â”œâ”€â”€ citation_parser.py # Rule-based citation parser
â”‚           â”‚   â”œâ”€â”€ cts_catalog.py     # CTS URN catalog management
â”‚           â”‚   â””â”€â”€ fuzzy_matcher.py   # Fuzzy matching for abbreviations
â”‚           â”œâ”€â”€ models/                # URN resolution models
â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚           â”‚   â”œâ”€â”€ seq2seq.py         # T5/BART seq2seq models
â”‚           â”‚   â”œâ”€â”€ retrieval.py       # RAG-based retrieval models
â”‚           â”‚   â””â”€â”€ classifier.py      # Ambiguity resolution classifier
â”‚           â”œâ”€â”€ training/              # Training logic
â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚           â”‚   â””â”€â”€ trainer.py         # Training loops
â”‚           â””â”€â”€ evaluation/            # Evaluation and metrics
â”‚               â”œâ”€â”€ __init__.py
â”‚               â”œâ”€â”€ metrics.py         # URN accuracy metrics
â”‚               â””â”€â”€ analysis.py        # Error analysis utilities
â”‚
â”œâ”€â”€ scripts/                           # Executable scripts
â”‚   â”œâ”€â”€ extraction/                    # Tag extraction scripts
â”‚   â”‚   â”œâ”€â”€ prepare_data.py            # Convert JSONL â†’ BIO format
â”‚   â”‚   â”œâ”€â”€ train_transformer.py       # Train DeBERTa token classifier
â”‚   â”‚   â”œâ”€â”€ train_transformer_crf.py   # Train transformer+CRF model
â”‚   â”‚   â”œâ”€â”€ evaluate.py                # Run evaluation on test set
â”‚   â”‚   â”œâ”€â”€ predict.py                 # Tag extraction inference
â”‚   â”‚   â””â”€â”€ analyze_errors.py          # Error analysis
â”‚   â””â”€â”€ resolution/                    # URN resolution scripts
â”‚       â”œâ”€â”€ analyze_citations.py       # Analyze citation patterns
â”‚       â”œâ”€â”€ build_catalog.py           # Build CTS catalog from data
â”‚       â”œâ”€â”€ train_parser.py            # Train/test rule-based parser
â”‚       â”œâ”€â”€ train_author_classifier.py # Train author classification model
â”‚       â”œâ”€â”€ train_work_classifier.py   # Train work classification model
â”‚       â”œâ”€â”€ evaluate.py                # Evaluate URN resolution
â”‚       â””â”€â”€ resolve_batch.py           # Batch URN resolution
â”‚
â”œâ”€â”€ configs/                           # Configuration files
â”‚   â”œâ”€â”€ base_config.yaml               # Shared config (paths, data locations)
â”‚   â”œâ”€â”€ extraction/                    # Tag extraction configs
â”‚   â”‚   â”œâ”€â”€ deberta_base.yaml          # DeBERTa hyperparams (recommended)
â”‚   â”‚   â”œâ”€â”€ roberta_base.yaml          # RoBERTa hyperparams
â”‚   â”‚   â”œâ”€â”€ bert_base.yaml             # BERT baseline hyperparams
â”‚   â”‚   â””â”€â”€ transformer_crf.yaml       # Transformer+CRF hyperparams
â”‚   â””â”€â”€ resolution/                    # URN resolution configs
â”‚       â”œâ”€â”€ hybrid_parser.yaml         # Rule-based + ML hybrid config
â”‚       â”œâ”€â”€ author_classifier.yaml     # Author classification hyperparams
â”‚       â”œâ”€â”€ work_classifier.yaml       # Work classification hyperparams
â”‚       â””â”€â”€ confidence_scorer.yaml     # Confidence scoring hyperparams
â”‚
â”œâ”€â”€ notebooks/                         # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb      # EDA on resolved.jsonl and unresolved.jsonl
â”‚   â”œâ”€â”€ extraction/                    # Tag extraction notebooks
â”‚   â”‚   â”œâ”€â”€ 02_preprocessing_test.ipynb    # Test BIO tagging pipeline
â”‚   â”‚   â”œâ”€â”€ 03_model_experiments.ipynb     # Quick tag extraction experiments
â”‚   â”‚   â””â”€â”€ 04_error_analysis.ipynb        # Analyze tagging mistakes
â”‚   â””â”€â”€ resolution/                    # URN resolution notebooks
â”‚       â”œâ”€â”€ 05_citation_analysis.ipynb     # Analyze citation patterns
â”‚       â”œâ”€â”€ 06_parser_experiments.ipynb    # Test parsing strategies
â”‚       â””â”€â”€ 07_urn_error_analysis.ipynb    # Analyze resolution errors
â”‚
â”œâ”€â”€ outputs/                           # Training outputs (gitignored)
â”‚   â”œâ”€â”€ extraction/                    # Tag extraction outputs
â”‚   â”‚   â”œâ”€â”€ models/                    # Saved model checkpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ deberta-base-run1/
â”‚   â”‚   â”‚   â”œâ”€â”€ roberta-base-run1/
â”‚   â”‚   â”‚   â””â”€â”€ transformer-crf-run1/
â”‚   â”‚   â”œâ”€â”€ logs/                      # Training logs, tensorboard
â”‚   â”‚   â”‚   â””â”€â”€ runs/
â”‚   â”‚   â”œâ”€â”€ predictions/               # Model predictions
â”‚   â”‚   â”‚   â””â”€â”€ test_predictions.jsonl
â”‚   â”‚   â””â”€â”€ metrics/                   # Evaluation results
â”‚   â”‚       â””â”€â”€ results_summary.json
â”‚   â””â”€â”€ resolution/                    # URN resolution outputs
â”‚       â”œâ”€â”€ models/                    # Saved model checkpoints
â”‚       â”‚   â”œâ”€â”€ author-classifier-run1/
â”‚       â”‚   â”œâ”€â”€ work-classifier-run1/
â”‚       â”‚   â”œâ”€â”€ confidence-scorer-run1/
â”‚       â”‚   â””â”€â”€ hybrid-system-run1/
â”‚       â”œâ”€â”€ catalogs/                  # CTS catalog builds
â”‚       â”‚   â””â”€â”€ cts_catalog.json
â”‚       â”œâ”€â”€ predictions/               # URN predictions
â”‚       â”‚   â””â”€â”€ resolved_urns.jsonl
â”‚       â””â”€â”€ metrics/                   # Evaluation results
â”‚           â””â”€â”€ results_summary.json
â”‚
â”œâ”€â”€ tests/                             # Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_shared.py                 # Test shared utilities
â”‚   â”œâ”€â”€ extraction/                    # Tag extraction tests
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_xml_parser.py
â”‚   â”‚   â”œâ”€â”€ test_models.py
â”‚   â”‚   â””â”€â”€ test_dataset.py
â”‚   â””â”€â”€ resolution/                    # URN resolution tests
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ test_citation_parser.py
â”‚       â”œâ”€â”€ test_catalog.py
â”‚       â””â”€â”€ test_models.py
â”‚
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ pyproject.toml                     # Project configuration
â”œâ”€â”€ .gitignore                         # Git ignore (outputs/, etc.)
â””â”€â”€ README.md                          # Project documentation
```

---

## End-to-End Pipeline

The two tasks can be combined into a complete citation processing pipeline:

```
Raw Text â†’ Tag Extraction â†’ URN Resolution â†’ Structured Citations
```

**Example workflow:**

1. **Input**: "Homer mentions this in Il. 7.268-272: 'Ajax hurled a rock'"
2. **Tag Extraction**: Identify `<bibl>Il. 7.268-272</bibl>` and `<quote>Ajax hurled a rock</quote>`
3. **URN Resolution**: Map "Il. 7.268-272" â†’ "urn:cts:greekLit:tlg0012.tlg001.perseus-grc2:7.268"
4. **Output**: Structured citation with linked canonical reference

This enables:

- Automated citation extraction from plain text
- Linking to canonical text passages
- Cross-referencing across documents
- Building citation networks in classical scholarship

---

## Useful Links

**General:**

- TEI Guidelines: <https://tei-c.org/release/doc/tei-p5-doc/en/html/>
- CTS URN Specification: <http://cite-architecture.github.io/cts_spec/>
- Perseus Digital Library: <https://www.perseus.tufts.edu/>

**Tag Extraction:**

- HuggingFace Token Classification: <https://huggingface.co/docs/transformers/tasks/token_classification>
- pytorch-crf: <https://github.com/kmkurn/pytorch-crf>
- seqeval metrics: <https://github.com/chakki-works/seqeval>
- DeBERTa paper: <https://arxiv.org/abs/2006.03654>

**URN Resolution:**

- DeBERTa paper: <https://arxiv.org/abs/2006.03654>
- Hierarchical classification: <https://arxiv.org/abs/1904.02817>
- CTS URN Specification: <http://cite-architecture.github.io/cts_spec/>
- CTS API documentation: <http://cite-architecture.github.io/cts/>
- perseus-citation-processor: <https://github.com/andrewbird2/perseus-citation-processor>
- Fuzzy string matching: <https://github.com/seatgeek/fuzzywuzzy>
